import os
import pandas as pd
from tracking import load_location_tracker, update_log
from fetch_product import fetch_products_in_batches
from kroger_api import get_kroger_product_compact_token
from data_processing import filter_products, save_to_csv

# âœ… Set up directory paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/` directory
DATA_DIR = os.path.join(BASE_DIR, "data")

# âœ… Define file paths
PRODUCTS_FILE = os.path.join(DATA_DIR, "kroger_product_data.csv")
PRODUCT_API_LOG = os.path.join(DATA_DIR, "product_api_log.csv")

def main(batch_size=10):
    """Orchestrates the full Kroger data pipeline."""
    
    print("\nğŸš€ **Starting Kroger Data Pipeline** ğŸš€\n")

    # âœ… Step 1: Ensure API Token is Valid
    token = get_kroger_product_compact_token()
    if not token:
        print("âŒ Failed to retrieve API token. Exiting...")
        return
    
    # âœ… Step 2: Initialize & Update Logs
    print("ğŸ”„ Loading tracker & checking logs...")
    load_location_tracker()
    update_log()  # Mark locations that need updates

    # âœ… Step 3: Fetch & Process Product Data
    print("ğŸ“¦ Fetching product data...")
    fetch_products_in_batches(batch_size=batch_size)

    # âœ… Step 4: Load & Filter Data
    print("ğŸ› ï¸ Processing and filtering product data...")
    if os.path.exists(PRODUCTS_FILE):
        df = pd.read_csv(PRODUCTS_FILE)
        filtered_df = filter_products(df)
        save_to_csv(filtered_df, PRODUCTS_FILE)
        print(f"âœ… Processed and saved filtered product data to {PRODUCTS_FILE}")
    else:
        print("âš ï¸ No product data file found! Skipping filtering step.")

    # âœ… Step 5: Final Log Update
    update_log()
    
    print("\nğŸ‰ **Pipeline Execution Complete!** ğŸ‰")

if __name__ == "__main__":
    main(batch_size=2)  # Adjust batch size as needed
